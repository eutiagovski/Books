{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendizado Profundo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = list\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def shape(tensor: Tensor) -> List[int]:\n",
    "    sizes: List[int] = []\n",
    "    while isinstance(tensor, list):\n",
    "        sizes.append(len(tensor))\n",
    "        tensor = tensor[0]\n",
    "\n",
    "    return sizes\n",
    "\n",
    "assert shape([1, 2, 3]) == [3]\n",
    "assert shape([[1, 2], [3, 4], [5, 6]]) == [3, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_1d(tensor: Tensor) -> bool:\n",
    "    \"\"\"\n",
    "    Se o tensor [0] é uma lista, é um tensor de ordem superior.\n",
    "    Se não, o tensor é unidimensional (ou seja, um vetor).\n",
    "    \"\"\"\n",
    "    return not isinstance(tensor[0], list)\n",
    "\n",
    "assert is_1d([1, 2, 3])\n",
    "assert not is_1d([[1, 2], [3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_sum(tensor: Tensor) -> float:\n",
    "    \"\"\"Soma todos os valores do tensor\"\"\"\n",
    "    if is_1d(tensor):\n",
    "        return sum(tensor)                  # apenas uma lista de floats, use a soma Python\n",
    "    else:\n",
    "        return sum(tensor_sum(tensor_i)     # Chame tensor_sum em cada linha\n",
    "                   for tensor_i in tensor)  # e some esses resultados\n",
    "\n",
    "assert tensor_sum([1, 2, 3]) == 6\n",
    "assert tensor_sum([[1, 2], [3, 4]]) == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def tensor_apply(f: Callable[[float], float], tensor: Tensor) -> Tensor:\n",
    "    \"\"\"Aplica f elementwise\"\"\"\n",
    "    if is_1d(tensor):\n",
    "        return [f(x) for x in tensor]\n",
    "    else:\n",
    "        return [tensor_apply(f, tensor_i) for tensor_i in tensor]\n",
    "\n",
    "assert tensor_apply(lambda x: x + 1, [1, 2, 3]) == [2, 3, 4]\n",
    "assert tensor_apply(lambda x: 2 * x, [[1, 2], [3, 4]]) == [[2, 4], [6, 8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeros_like(tensor: Tensor) -> Tensor:\n",
    "    return tensor_apply(lambda _: 0.0, tensor)\n",
    "\n",
    "assert zeros_like([1, 2, 3]) == [0, 0, 0]\n",
    "assert zeros_like([[1, 2], [3, 4]]) == [[0, 0], [0, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_combine(f: Callable[[float, float], float],\n",
    "                   t1: Tensor,\n",
    "                   t2: Tensor) -> Tensor:\n",
    "    \"\"\"Aplica f aos elementos correspondentes de t1 e t2\"\"\"\n",
    "    if is_1d(t1):\n",
    "        return [f(x, y) for x, y in zip(t1, t2)]\n",
    "    else:\n",
    "        return [tensor_combine(f, t1_i, t2_i)\n",
    "                for t1_i, t2_i in zip(t1, t2)]\n",
    "\n",
    "import operator\n",
    "assert tensor_combine(operator.add, [1, 2, 3], [4, 5, 6]) == [5, 7, 9]\n",
    "assert tensor_combine(operator.mul, [1, 2, 3], [4, 5, 6]) == [4, 10, 18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstração de Camadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Tuple\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    Nossas redes neurais serão compostas por Layers que sabem\n",
    "    computar as entradas \"pra frente\" e propagar gradientes \"para trás\".\n",
    "    \"\"\"\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Observe que não há tipos. Não indicaremos expressamente os\n",
    "        tipos de entradas que serão recebidas pelas camadas nem os\n",
    "        tipos de saídas que elas retornarão.\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        \"\"\"\n",
    "        Da mesma forma, não indicaremos expressamente o formato do\n",
    "        gradiente. Cabe ao usuário (você) avaliar se está fazendo as\n",
    "        coisas de forma razoável.\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "    def params(self):\n",
    "        \"\"\"\n",
    "        Retorna os parâmetros desta camada. Como a implementação padrão\n",
    "        não retorna nada, se houver uma camada sem parâmetros, você não\n",
    "        precisará implementar isso.\n",
    "        \"\"\"\n",
    "        return ()\n",
    "    \n",
    "    def grads(self) -> Iterable[Tensor]:\n",
    "        \"\"\"\n",
    "        Retorna os gradientes na mesma ordem dos params().\n",
    "        \"\"\"\n",
    "        return ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from neural_networks import sigmoid\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Aplique a sigmoid em todos os elementos do tensor de \n",
    "        entrada e salve os resultados para usar na retropropagação.\n",
    "        \"\"\"\n",
    "        self.sigmoids = tensor_apply(sigmoid, input)\n",
    "        return self.sigmoids\n",
    "    \n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        return tensor_combine(lambda sig, grad: sig * (1 - sig) * grad,\n",
    "                              self.sigmoids,\n",
    "                              gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Camada Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from probability import inverse_normal_cdf\n",
    "\n",
    "def random_uniform(*dims: int) -> Tensor:\n",
    "    if len(dims) == 1:\n",
    "        return [random.random() for _ in range(dims[0])]\n",
    "    else:\n",
    "        return [random_uniform(*dims[1:]) for _ in range(dims[0])]\n",
    "\n",
    "def random_normal(*dims: int,\n",
    "                  mean: float = 0.0,\n",
    "                  variance: float = 1.0) -> Tensor:\n",
    "    if len(dims) == 1:\n",
    "        return [mean + variance * inverse_normal_cdf(random.random())\n",
    "                for _ in range(dims[0])]\n",
    "    else:\n",
    "        return [random_normal(*dims[1:], mean=mean, variance=variance)\n",
    "                for _ in range(dims[0])]\n",
    "    \n",
    "assert shape(random_uniform(2, 3, 4)) == [2, 3, 4]\n",
    "assert shape(random_normal(5, 6, mean=10)) == [5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_tensor(*dims: int, init: str = 'normal') -> Tensor:\n",
    "    if init == 'normal':\n",
    "        return random_normal(*dims)\n",
    "    elif init == 'uniform':\n",
    "        return random_uniform(*dims)\n",
    "    elif init == 'xavier':\n",
    "        variance = len(dims) / sum(dims)\n",
    "        return random_normal(*dims, variance=variance)\n",
    "    else:\n",
    "        raise ValueError(f'unknown init: {init}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_algebra import dot\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, input_dim: int, output_dim: int, init: str = 'xavier') -> None:\n",
    "        \"\"\"\n",
    "        A layer of output_dim neurons, each with input_dim weights\n",
    "        (and a bias).\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # self.w[o] is the weights for the o-th neuron\n",
    "        self.w = random_tensor(output_dim, input_dim, init=init)\n",
    "\n",
    "        # self.b[o] is the bias term for the o-th neuron\n",
    "        self.b = random_tensor(output_dim, init=init)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        # Save the input to use in the backward pass.\n",
    "        self.input = input\n",
    "\n",
    "        # Return the vector of neuron outputs.\n",
    "        return [dot(input, self.w[o]) + self.b[o]\n",
    "                for o in range(self.output_dim)]\n",
    "\n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        # Each b[o] gets added to output[o], which means\n",
    "        # the gradient of b is the same as the output gradient.\n",
    "        self.b_grad = gradient\n",
    "\n",
    "        # Each w[o][i] multiplies input[i] and gets added to output[o].\n",
    "        # So its gradient is input[i] * gradient[o].\n",
    "        self.w_grad = [[self.input[i] * gradient[o]\n",
    "                        for i in range(self.input_dim)]\n",
    "                       for o in range(self.output_dim)]\n",
    "\n",
    "        # Each input[i] multiplies every w[o][i] and gets added to every\n",
    "        # output[o]. So its gradient is the sum of w[o][i] * gradient[o]\n",
    "        # across all the outputs.\n",
    "        return [sum(self.w[o][i] * gradient[o] for o in range(self.output_dim))\n",
    "                for i in range(self.input_dim)]\n",
    "\n",
    "    def params(self) -> Iterable[Tensor]:\n",
    "        return [self.w, self.b]\n",
    "\n",
    "    def grads(self) -> Iterable[Tensor]:\n",
    "        return [self.w_grad, self.b_grad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redes Neurais Como Sequencias de Camadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class Sequential(Layer):\n",
    "    \"\"\"\n",
    "    Uma camada é uma sequência de outras camadas.\n",
    "    Cabe a você avaliar se há coerência entre a saída de uma camada \n",
    "    e a entrada da próxima camada.\n",
    "    \"\"\"\n",
    "    def __init__(self, layers: List[Layer]) -> None:\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Só avance a entrada pelas camadas em sequência.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        \n",
    "        return input\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        \"\"\"Só retropropague o gradient pelas camadas na sequência inversa.\"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            gradient = layer.backward(gradient)\n",
    "        return gradient\n",
    "\n",
    "    def params(self) -> Iterable[Tensor]:\n",
    "        \"\"\"Só retorne os params de cada camada.\"\"\"\n",
    "        return (param for layer in self.layers for param in layer.params())\n",
    "\n",
    "    def grads(self) -> Iterable[Tensor]:\n",
    "        \"\"\"Só retorne os grads de cada camada.\"\"\"\n",
    "        return (grad for layer in self.layers for grad in layer.grads())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represenando a rede neural que usamos para XOR\n",
    "\n",
    "xor_net = Sequential([\n",
    "    Linear(input_dim=2, output_dim=2),\n",
    "    Sigmoid(),\n",
    "    Linear(input_dim=2, output_dim=1),\n",
    "    Sigmoid()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perda e Otimização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
    "        \"\"\"Qual é a qualidade das previsões? (Os números maiores são piores).\"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
    "        \"\"\"Como a perda muda a medida que mudam as previsões?\"\"\"\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSE(Loss):\n",
    "    \"\"\"A função da perda que computa a soma dos erros quadráticos.\"\"\"\n",
    "    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
    "        # Compute o tensor das diferenças quadráticas\n",
    "        squared_errors = tensor_combine(\n",
    "            lambda predicted, actual: (predicted - actual) ** 2,\n",
    "            predicted,\n",
    "            actual\n",
    "        )\n",
    "\n",
    "        # E some tudo\n",
    "        return tensor_sum(squared_errors)\n",
    "\n",
    "    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
    "        return tensor_combine(\n",
    "            lambda predicted, actual: 2 * (predicted - actual),\n",
    "            predicted,\n",
    "            actual\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"\n",
    "    O otimizador atualiza os pesos de uma camada (no local) usando informações\n",
    "    conhecidas pela camada ou pelo otimizador (ou por ambos).\n",
    "    \"\"\"\n",
    "    def step(self, layer: Layer) -> None:\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent(Optimizer):\n",
    "    def __init__(self, learning_rate: float = 0.1) -> None:\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def step(self, layer: Layer) -> None:\n",
    "        for param, grad in zip(layer.params(), layer.grads()):\n",
    "            # Atualize o param utilizando um passo de gradiente\n",
    "            param[:] = tensor_combine(\n",
    "                lambda param, grad: param - grad * self.lr,\n",
    "                param,\n",
    "                grad\n",
    "            )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = [[1, 2], [3, 4]]\n",
    "\n",
    "for row in tensor:\n",
    "    row = [0, 0]\n",
    "assert tensor == [[1, 2], [3, 4]], 'a atribuição não atualiza a lista'\n",
    "\n",
    "for row in tensor:\n",
    "    row[:] = [0, 0]\n",
    "assert tensor == [[0, 0], [0, 0]], 'mas a atribuição de fatia sim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum(Optimizer):\n",
    "    def __init__(self, learning_rate: float, momentum: float = 0.9) -> None:\n",
    "        self.lr = learning_rate\n",
    "        self.mo = momentum\n",
    "        self.updates: List[Tensor] = []     # média móvel\n",
    "\n",
    "    def step(self, layer: Layer) -> None:\n",
    "        # Se não houver atualizações anteriores, começe com zeros\n",
    "        if not self.updates:\n",
    "            self.updates = [zero_likes(grad) for grad in layer.grads()]\n",
    "        \n",
    "        for update, param, grad in zip(self.updates, layer.params(), layer.grads()):\n",
    "            # Aplique o momentum\n",
    "            update[:] = tensor_combine(\n",
    "                lambda u, g: self.mo * u + (1 - self.mo) * g,\n",
    "                update,\n",
    "                grad\n",
    "            )\n",
    "\n",
    "            # Em seguida, dê um passo de gradiente\n",
    "            param[:] = tensor_combine(\n",
    "                lambda p, u: p - self.lr * u,\n",
    "                param,\n",
    "                update\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [[0., 0], [0., 1], [1., 0], [1., 1]]\n",
    "ys = [[0.], [1.], [1.], [0.]]\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "net = Sequential([\n",
    "    Linear(input_dim=2, output_dim=2),\n",
    "    Sigmoid(),\n",
    "    Linear(input_dim=2, output_dim=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xor loss 0.000: 100%|██████████| 3000/3000 [00:10<00:00, 283.17it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "optimizer = GradientDescent(learning_rate=0.1)\n",
    "loss = SSE()\n",
    "\n",
    "with tqdm.trange(3000) as t:\n",
    "    for epoch in t:\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for x, y in zip(xs, ys):\n",
    "            predicted = net.forward(x)\n",
    "            epoch_loss += loss.loss(predicted, y)\n",
    "            gradient = loss.gradient(predicted, y)\n",
    "            net.backward(gradient)\n",
    "\n",
    "            optimizer.step(net)\n",
    "\n",
    "        t.set_description(f\"xor loss {epoch_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.6425160695224095, -1.4948117798303144], [-4.5676465720296635, -3.3649176350731893]]\n",
      "[1.7673716823255186, 0.387270143794725]\n",
      "[[3.198620479170404, -3.501803062142621]]\n",
      "[-0.6462765963362219]\n"
     ]
    }
   ],
   "source": [
    "for param in net.params():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
